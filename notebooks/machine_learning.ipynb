{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML with PySpark\n",
    "\n",
    "The primary machine learning API is contained in `pyspark.ml`. It is based on PySpark DataFrames. The well-known RDD-based API `pyspark.mllib` is mainly provided for legacy purposes and might become deprecated at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    OneHotEncoder,\n",
    "    VectorAssembler\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from utils import download_textfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Logistic Regression') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download CSV file\n",
    "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
    "\n",
    "download_textfile(url, save_path='../data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CSV file\n",
    "df = spark.read.csv('../data/titanic.csv', header=True, inferSchema=True)\n",
    "\n",
    "print(f'The data has {df.count()} rows and {len(df.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary\n",
    "df.show(10)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaNs\n",
    "df = df.na.drop() # note that there are no NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "columns = [\n",
    "    'Survived',\n",
    "    'Pclass',\n",
    "    'Sex',\n",
    "    'Age',\n",
    "    'Siblings/Spouses Aboard',\n",
    "    'Parents/Children Aboard',\n",
    "    'Fare'\n",
    "]\n",
    "\n",
    "df = df.select(columns) # df = df.drop('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformations\n",
    "sex_indexer = StringIndexer(inputCol='Sex', outputCol='SexIdx')\n",
    "sex_encoder = OneHotEncoder(inputCol='SexIdx', outputCol='SexVec')\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'Survived',\n",
    "        'Pclass',\n",
    "        'SexVec',\n",
    "        'Age',\n",
    "        'Siblings/Spouses Aboard',\n",
    "        'Parents/Children Aboard',\n",
    "        'Fare'\n",
    "    ],\n",
    "    outputCol='Features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create estimator\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='Features',\n",
    "    labelCol='Survived',\n",
    "    maxIter=10,\n",
    "    regParam=0.3,\n",
    "    elasticNetParam=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "stages = [\n",
    "    sex_indexer,\n",
    "    sex_encoder,\n",
    "    assembler,\n",
    "    lr\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to test set\n",
    "results = model.transform(test_df)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with PySpark\n",
    "\n",
    "A short introduction to PySpark is provided in this notebook. It is merely a starting point for exploring core features such as PySpark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "The main entry point for a PySpark application is provided by `SparkSession`. Before the unification that has been introduced in Spark 2.0, `SparkContext` used to provide one of three different starting points. A session object can be initialized as it is shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('TestApp') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "A PySpark `DataFrame` can be created in different ways, for example through a `pd.DataFrame` or a list of rows and and an explicit schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe\n",
    "pandas_df = pd.DataFrame(\n",
    "    {\n",
    "        'a': [1, 2, 3],\n",
    "        'b': [2., 3., 4.],\n",
    "        'c': ['string1', 'string2', 'string3'],\n",
    "        'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "        'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "    }\n",
    ")\n",
    "\n",
    "# create dataframe from pandas dataframe\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# transform back to pandas\n",
    "# pandas_df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark dataframe from list of rows\n",
    "spark_df = spark.createDataFrame(\n",
    "    [\n",
    "        Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "        Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "        Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark dataframe with an explicit schema\n",
    "spark_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "        (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "        (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "    ],\n",
    "    schema='a long, b double, c string, d date, e timestamp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns (note that dataframes are lazily evaluated)\n",
    "one_col = spark_df.a\n",
    "\n",
    "two_cols = spark_df.select('a', 'b')\n",
    "two_cols = spark_df['a', 'b']\n",
    "\n",
    "print(one_col)\n",
    "print(two_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the first rows\n",
    "list_of_first_rows = spark_df.head(2) # list_of_first_rows = spark_df.take(2)\n",
    "list_of_last_rows = spark_df.tail(2)\n",
    "\n",
    "print(list_of_first_rows)\n",
    "print(list_of_last_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect distributed data to the driver (note that this may cause an out-of-memory error)\n",
    "list_of_all_rows = spark_df.collect()\n",
    "\n",
    "print(len(list_of_all_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows of dataframe\n",
    "filtered_df = spark_df.filter(spark_df.a == 1)\n",
    "\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ['red', 'banana', 1, 10],\n",
    "        ['blue', 'banana', 2, 20],\n",
    "        ['red', 'carrot', 3, 30],\n",
    "        ['blue', 'grape', 4, 40],\n",
    "        ['red', 'carrot', 5, 50],\n",
    "        ['black', 'carrot', 6, 60],\n",
    "        ['red', 'banana', 7, 70],\n",
    "        ['red', 'grape', 8, 80]\n",
    "    ],\n",
    "    schema=['color', 'fruit', 'v1', 'v2']\n",
    ")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register dataframe as SQL table\n",
    "df.createOrReplaceTempView('tableA')\n",
    "\n",
    "# run SQL-style query\n",
    "spark.sql('SELECT count(*) from tableA').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
